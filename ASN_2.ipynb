{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJmxtD-kg8KE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SubstringDataset(Dataset):\n",
        "    LETTERS = list('cpen')\n",
        "\n",
        "    def __init__(self, seed, dataset_size, str_len=20):\n",
        "        super().__init__()\n",
        "        self.str_len = str_len\n",
        "        self.dataset_size = dataset_size\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.strings, self.labels = self._create_dataset()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.strings[index], self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def _create_dataset(self):\n",
        "        strings, labels = [], []\n",
        "        for i in range(self.dataset_size):\n",
        "            label = i%2\n",
        "            string = self._generate_random_string(bool(label))\n",
        "            strings.append(string)\n",
        "            labels.append(label)\n",
        "        return strings, labels\n",
        "\n",
        "    def _generate_random_string(self, has_cpen):\n",
        "        while True:\n",
        "            st = ''.join(self.rng.choice(SubstringDataset.LETTERS, size=self.str_len))\n",
        "            if ('cpen' in st) == has_cpen:\n",
        "                return st"
      ],
      "metadata": {
        "id": "Eritefi9hPgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "    def __init__(self) -> None:\n",
        "        self.vocab = {\n",
        "            '[CLS]': 0,\n",
        "            'c': 1,\n",
        "            'p': 2,\n",
        "            'e': 3,\n",
        "            'n': 4,\n",
        "        }\n",
        "\n",
        "    def tokenize_string(self, string, add_cls_token=True) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Tokenize the input string according to the above vocab\n",
        "\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        #prepend [CLS] to string\n",
        "        tokenized_string = list(string)\n",
        "        if add_cls_token:\n",
        "          tokenized_string = ['[CLS]'] + tokenized_string\n",
        "\n",
        "        #copy into array\n",
        "        indices = []\n",
        "        for token in tokenized_string:\n",
        "          indices.append(self.vocab[token])\n",
        "\n",
        "        #convert onehot (n+1, 5) matrix (looks like a diagonal)\n",
        "        tokenized_string = torch.eye(5)[indices]\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return tokenized_string\n",
        "\n",
        "    def tokenize_string_batch(self, strings, add_cls_token=True):\n",
        "        X = []\n",
        "        for s in strings:\n",
        "            X.append(self.tokenize_string(s, add_cls_token=add_cls_token))\n",
        "        return torch.stack(X, dim=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "AY91aQytmtqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AbsolutePositionalEncoding(nn.Module):\n",
        "    MAX_LEN = 256\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.empty((self.MAX_LEN, d_model)))\n",
        "        nn.init.normal_(self.W)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            x: shape B x N x D\n",
        "        returns:\n",
        "            out: shape B x N x D\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        #get sequence length N, choose corresponding position\n",
        "        B, N, D = x.shape\n",
        "\n",
        "        #self.W is trainable parameter matrix, we choose N rows from it (for each N tokens)\n",
        "        position_encoding = self.W[:N]\n",
        "\n",
        "        out = x + position_encoding\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    MAX_LEN = 256\n",
        "\n",
        "    def __init__(self, d_model, n_heads, rpe):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"Number of heads must divide number of dimensions\"\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_h = d_model // n_heads\n",
        "        self.rpe = rpe\n",
        "        self.Wq = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
        "        self.Wk = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
        "        self.Wv = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
        "        self.Wo = nn.Parameter(torch.empty((d_model, d_model)))\n",
        "\n",
        "        if rpe:\n",
        "            # -MAX_LEN, -MAX_LEN+1, ..., -1, 0, 1, ..., MAX_LEN-1, MAXLEN\n",
        "            self.rpe_w = nn.ParameterList([nn.Parameter(torch.empty((2*self.MAX_LEN+1, ))) for _ in range(n_heads)])\n",
        "\n",
        "        for h in range(self.n_heads):\n",
        "            nn.init.xavier_normal_(self.Wk[h])\n",
        "            nn.init.xavier_normal_(self.Wq[h])\n",
        "            nn.init.xavier_normal_(self.Wv[h])\n",
        "            if rpe:\n",
        "                nn.init.normal_(self.rpe_w[h])\n",
        "        nn.init.xavier_normal_(self.Wo)\n",
        "\n",
        "    def forward(self, key, query, value):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            key: shape B x N x D\n",
        "            query: shape B x N x D\n",
        "            value: shape B x N x D\n",
        "        return:\n",
        "            out: shape B x N x D\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        #key, query, and value are same shape\n",
        "        B, N, D = key.shape\n",
        "\n",
        "        attention = []\n",
        "        for h in range(self.n_heads):\n",
        "          #get projections\n",
        "          Q = torch.matmul(query, self.Wq[h]) #(B,N,D) x (D,d_h) -> (B,N,d_h)\n",
        "          K = torch.matmul(key, self.Wk[h])\n",
        "          V = torch.matmul(value, self.Wv[h])\n",
        "\n",
        "          #\"swap\" 2nd last and last dimensions (N <-> d_h)\n",
        "          scores = torch.matmul(Q, K.transpose(-2, -1)) #(B,N,D) x (B,d_h,N) -> (B,N,N)\n",
        "\n",
        "          if self.rpe:\n",
        "            N = Q.size(-2) #Q[-2]=N\n",
        "            pos_vector = torch.arange(N) #[0,1,...,N-1]\n",
        "\n",
        "            #find relative position differences with Toeplitz (N,N)\n",
        "            #unsqueeze(0): (1,N), unsqueeze(1): (N,1)\n",
        "            #[[0-0,1-0,2-0],   [[ 0, 1,2],\n",
        "            # [0-1,1-1,2-1], =  [-1, 0,1],\n",
        "            # [0-2,1-2,2-2]]    [-2,-1,0]]\n",
        "            rpd_matrix = pos_vector.unsqueeze(0) - pos_vector.unsqueeze(1)\n",
        "            rpd_matrix = rpd_matrix + self.MAX_LEN #make sure that range is positive (-N+1...N-1) -> (0...2*MAX_LEN)\n",
        "            bias = self.rpe_w[h][rpd_matrix] #creates diagonal 0s with negatives down and positives top\n",
        "            scores = scores + bias\n",
        "\n",
        "          scores = scores / torch.sqrt(torch.tensor(self.d_h))\n",
        "          attention_weights = F.softmax(scores, dim=-1) #dim=-1 implies row-wise\n",
        "\n",
        "          #store heads\n",
        "          heads = torch.matmul(attention_weights, V)\n",
        "          attention.append(heads)\n",
        "\n",
        "        #we are then concatinating the heads\n",
        "        ccat = torch.cat(attention, dim=-1)\n",
        "\n",
        "        #multiply by weight for attention\n",
        "        out = torch.matmul(ccat, self.Wo)\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "AG_mI9VSj0XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, prenorm: bool, rpe: bool):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.prenorm = prenorm\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads, rpe=rpe)\n",
        "        self.fc_W1 = nn.Parameter(torch.empty((d_model, 4*d_model)))\n",
        "        self.fc_W2 = nn.Parameter(torch.empty((4*d_model, d_model)))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        nn.init.xavier_normal_(self.fc_W1)\n",
        "        nn.init.xavier_normal_(self.fc_W2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            x: shape B x N x D\n",
        "        returns:\n",
        "            out: shape B x N x D\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        #pre-norm:\n",
        "        #Norm>Attention>Residual>Norm>Feedforward>Residual\n",
        "        #x+=attention(x), x+=feedforward(x)\n",
        "        if self.prenorm:\n",
        "          #multi-head self-attention\n",
        "          x_norm = self.ln1(x) #pre-norm\n",
        "          attention = self.attention(x_norm, x_norm, x_norm) #self-attention\n",
        "          x = x + attention #residual connection\n",
        "\n",
        "          #feedforward layer\n",
        "          x_norm = self.ln2(x)\n",
        "          prod = torch.matmul(x_norm, self.fc_W1) #XW_1\n",
        "          relu = torch.maximum(prod, torch.zeros_like(prod))\n",
        "          feedforward = torch.matmul(relu, self.fc_W2) #ReLU(XW_1)W_2\n",
        "          out = x + feedforward #residual connection\n",
        "\n",
        "        #post-norm:\n",
        "        #Attention>Residual>Norm>Feedforward>Residual>Norm\n",
        "        #x+=attention(x), x+=feedforward(x)\n",
        "        else:\n",
        "          #multi-head self-attention\n",
        "          attention = self.attention(x, x, x) #self-attention\n",
        "          x = x + attention #residual connection\n",
        "          x = self.ln1(x) #post-norm\n",
        "\n",
        "          #feedforward layer\n",
        "          prod = torch.matmul(x, self.fc_W1) #XW_1\n",
        "          relu = torch.maximum(prod, torch.zeros_like(prod))\n",
        "          feedforward = torch.matmul(relu, self.fc_W2) #ReLU(XW_1)W_2\n",
        "          x = x + feedforward #residual connection\n",
        "          out = self.ln2(x) #post-norm\n",
        "\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out"
      ],
      "metadata": {
        "id": "4iCqSqRDsGw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelConfig:\n",
        "    n_layers = 4\n",
        "    input_dim = 5\n",
        "    d_model = 256\n",
        "    n_heads = 4\n",
        "    prenorm = True\n",
        "    pos_enc_type = 'ape' # 'ape': Abosolute Pos. Enc., 'rpe': Relative Pos. Enc.\n",
        "    output_dim = 1 # Binary output: 0: invalid, 1: valid\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            assert hasattr(self, k)\n",
        "            self.__setattr__(k, v)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.enc_W = nn.Parameter(torch.empty((cfg.input_dim, cfg.d_model)))\n",
        "        if cfg.pos_enc_type == 'ape':\n",
        "            self.ape = AbsolutePositionalEncoding(d_model=cfg.d_model)\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model=cfg.d_model, n_heads=cfg.n_heads, prenorm=cfg.prenorm, rpe=cfg.pos_enc_type == 'rpe') for _ in range(cfg.n_layers)\n",
        "        ])\n",
        "        self.dec_W = nn.Parameter(torch.empty((cfg.d_model, cfg.output_dim)))\n",
        "\n",
        "        nn.init.xavier_normal_(self.enc_W)\n",
        "        nn.init.xavier_normal_(self.dec_W)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            x: shape B x N x D_in\n",
        "        returns:\n",
        "            out: shape B x N x D_out\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        #encode each D_in into D_model with W_enc\n",
        "        x = torch.matmul(x, self.enc_W) #(B,N,D_in) x (D_in,D_model) -> (B,N,D_model)\n",
        "\n",
        "        #absolute positional encoding\n",
        "        if self.cfg.pos_enc_type == 'ape':\n",
        "          x = self.ape(x)\n",
        "\n",
        "        #pass through nLayers transformer layers\n",
        "        for nLayers in self.transformer_layers:\n",
        "          x = nLayers(x) #stays (B,N,D_model)\n",
        "\n",
        "        #decode each D_model into D_out with W_dec\n",
        "        out = torch.matmul(x, self.dec_W) #(B,N,D_model) x (D_model,D_out) -> (B,N,D_out)\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out"
      ],
      "metadata": {
        "id": "IxPXjEj1ydLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import lr_scheduler\n",
        "\n",
        "class CustomScheduler(lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, total_steps, warmup_steps=1000):\n",
        "        self.total_steps = total_steps\n",
        "        self.warmup_steps = warmup_steps\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        \"\"\"\n",
        "        Compute the custom scheduler with warmup and cooldown\n",
        "        Hint: self.last_epoch contains the current step number\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        #warmup\n",
        "        #last_epoch->warmup_steps: mult_factor = linear 0 ->lr (last_epoch/warmup_steps)\n",
        "        if self.last_epoch < self.warmup_steps:\n",
        "          mult_factor = self.last_epoch / self.warmup_steps\n",
        "\n",
        "        #cooldown\n",
        "        #last_epoch->total_steps:  mult_factor = linear lr->0  ((total_steps - last_epoch)/(total_steps - warmup_steps))\n",
        "        elif self.last_epoch < self.total_steps:\n",
        "          mult_factor = (self.total_steps - self.last_epoch) / (self.total_steps - self.warmup_steps)\n",
        "\n",
        "        #last_epoch==0: mult_factor = 0\n",
        "        else:\n",
        "          mult_factor = 0\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return [group['initial_lr'] * mult_factor for group in self.optimizer.param_groups]"
      ],
      "metadata": {
        "id": "k32Ps5WS9rg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TrainerConfig:\n",
        "    lr = 0.003\n",
        "    train_steps = 5000\n",
        "    batch_size = 256\n",
        "    evaluate_every = 100\n",
        "    device = 'cpu'\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            assert hasattr(self, k)\n",
        "            self.__setattr__(k, v)\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, cfg: TrainerConfig):\n",
        "        self.cfg = cfg\n",
        "        self.device = cfg.device\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "    def train(self, train_dataset, val_dataset):\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.cfg.lr)\n",
        "        scheduler = CustomScheduler(optimizer, self.cfg.train_steps)\n",
        "        train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=self.cfg.batch_size)\n",
        "        for step in range(self.cfg.train_steps):\n",
        "            self.model.train()\n",
        "            batch = next(iter(train_dataloader))\n",
        "            strings, y = batch\n",
        "            x = self.tokenizer.tokenize_string_batch(strings)\n",
        "            x = x.to(self.device) #add to GPU\n",
        "            y = y.to(self.device) #add to GPU\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss, _ = self.compute_batch_loss_acc(x, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            if step % self.cfg.evaluate_every == 0:\n",
        "                val_loss, val_acc = self.evaluate_dataset(val_dataset)\n",
        "                print(f\"Step {step}: Train Loss={loss.item()}, Val Loss: {val_loss}, Val Accuracy: {val_acc}\")\n",
        "\n",
        "    def compute_batch_loss_acc(self, x, y):\n",
        "        \"\"\"\n",
        "        Compute the loss and accuracy of the model on batch (x, y)\n",
        "        args:\n",
        "            x: B x N x D_in\n",
        "            y: B\n",
        "        return:\n",
        "            loss, accuracy\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        #forward pass (B,N,1)\n",
        "        logits = self.model(x)\n",
        "\n",
        "        #get logits corresponing to [CLS] token\n",
        "        cls_logits = logits[:,0]\n",
        "        if cls_logits.dim() > 1:\n",
        "          cls_logits = cls_logits.squeeze(-1) #(B,)\n",
        "\n",
        "        #get cross entropy loss between [CLS] logits and ground truth y\n",
        "        loss = F.binary_cross_entropy_with_logits(cls_logits, y.float())\n",
        "\n",
        "        #predict class with sigmoid\n",
        "        predictions = (torch.sigmoid(cls_logits) > 0.5).int()\n",
        "\n",
        "        acc = (predictions == y.int()).float()\n",
        "        acc = acc.mean()\n",
        "        #loss, acc = torch.tensor([1.0]), torch.tensor([0.0])\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return loss, acc\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate_dataset(self, dataset):\n",
        "        self.model.eval()\n",
        "        dataloader = DataLoader(dataset, shuffle=False, batch_size=self.cfg.batch_size)\n",
        "        final_loss, final_acc = 0.0, 0.0\n",
        "        for batch in dataloader:\n",
        "            strings, y = batch\n",
        "            x = self.tokenizer.tokenize_string_batch(strings).to(self.device) #add to GPU\n",
        "            y = y.to(self.device) #add to GPU\n",
        "            loss, acc = self.compute_batch_loss_acc(x, y)\n",
        "            final_loss += loss.item() * x.size(0)\n",
        "            final_acc += acc.item() * x.size(0)\n",
        "        return final_loss / len(dataset), final_acc / len(dataset)\n"
      ],
      "metadata": {
        "id": "HmjFKAXcyeZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In case you were not successful in implementing some of the above classes,\n",
        "you may reimplement them using pytorch available nn Modules here to receive the marks for part 1.8\n",
        "If your implementation of the previous parts is correct, leave this block empty.\n",
        "START BLOCK\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "END BLOCK\n",
        "\"\"\"\n",
        "def run_transformer():\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = TransformerModel(ModelConfig()).to(device) #add to GPU\n",
        "    trainer = Trainer(model, TrainerConfig(device=device))\n",
        "    parantheses_size=16\n",
        "    print(\"Creating datasets.\")\n",
        "    train_dataset = SubstringDataset(seed=1, dataset_size=10_000, str_len=parantheses_size)\n",
        "    val_dataset = SubstringDataset(seed=2, dataset_size=1_000, str_len=parantheses_size)\n",
        "    test_dataset = SubstringDataset(seed=3, dataset_size=1_000, str_len=parantheses_size)\n",
        "\n",
        "    print(\"Training the model.\")\n",
        "    trainer.train(train_dataset, val_dataset)\n",
        "    test_loss, test_acc = trainer.evaluate_dataset(test_dataset)\n",
        "    print(f\"Final Test Accuracy={test_acc}, Test Loss={test_loss}\")"
      ],
      "metadata": {
        "id": "b5zfy4SVFy0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_transformer()"
      ],
      "metadata": {
        "id": "IhAUyeO5F27T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit Tests"
      ],
      "metadata": {
        "id": "IjNEOPRMsGKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def seed_all():\n",
        "    torch.manual_seed(0)\n",
        "    random.seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "class TransformerUnitTest:\n",
        "    def __init__(self, gt_vars: dict, verbose=False):\n",
        "        self.gt_vars = gt_vars\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def test_all(self):\n",
        "        self.test_tokenizer()\n",
        "        self.test_ape()\n",
        "        self.test_mha()\n",
        "        self.test_transformer_layer()\n",
        "        self.test_transformer_model()\n",
        "        self.test_scheduler()\n",
        "        self.test_loss()\n",
        "\n",
        "    def test_tokenizer(self):\n",
        "        seed_all()\n",
        "        self.check_correctness(\n",
        "            Tokenizer().tokenize_string('ccpeen', add_cls_token=True),\n",
        "            self.gt_vars['tokenizer_1'],\n",
        "            \"Tokenization with cls class\"\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            Tokenizer().tokenize_string('cpppencpen', add_cls_token=False),\n",
        "            self.gt_vars['tokenizer_2'],\n",
        "            \"Tokenization without cls class\"\n",
        "        )\n",
        "\n",
        "    def test_ape(self):\n",
        "        seed_all()\n",
        "        ape_result = AbsolutePositionalEncoding(128)(torch.randn((8, 12, 128)))\n",
        "        self.check_correctness(ape_result, self.gt_vars['ape'], \"APE\")\n",
        "\n",
        "    def test_mha(self):\n",
        "        seed_all()\n",
        "        mha_result = MultiHeadAttention(d_model=128, n_heads=4, rpe=False)(\n",
        "            torch.randn((8, 12, 128)), torch.randn((8, 12, 128)), torch.randn((8, 12, 128))\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            mha_result,\n",
        "            self.gt_vars['mha_no_rpe'],\n",
        "            \"Multi-head Attention without RPE\"\n",
        "        )\n",
        "        mha_result_rpe = MultiHeadAttention(d_model=128, n_heads=8, rpe=True)(\n",
        "            torch.randn((8, 12, 128)), torch.randn((8, 12, 128)), torch.randn((8, 12, 128))\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            mha_result_rpe,\n",
        "            self.gt_vars['mha_with_rpe'],\n",
        "            \"Multi-head Attention with RPE\"\n",
        "        )\n",
        "\n",
        "    def test_transformer_layer(self):\n",
        "        seed_all()\n",
        "        for prenorm in [True, False]:\n",
        "            transformer_layer_result = TransformerLayer(\n",
        "                d_model=128, n_heads=4, prenorm=prenorm, rpe=False\n",
        "            )(torch.randn((8, 12, 128)))\n",
        "            self.check_correctness(\n",
        "                transformer_layer_result,\n",
        "                self.gt_vars[f'transformer_layer_prenorm_{prenorm}'],\n",
        "                f\"Transformer Layer Prenorm {prenorm}\"\n",
        "            )\n",
        "\n",
        "    def test_transformer_model(self):\n",
        "        seed_all()\n",
        "        transformer_model_result = TransformerModel(\n",
        "            ModelConfig(d_model=128, prenorm=True, pos_enc_type='ape')\n",
        "        )(torch.randn((8, 12, 5)))\n",
        "        self.check_correctness(\n",
        "            transformer_model_result,\n",
        "            self.gt_vars['transformer_model_result'],\n",
        "            f\"Transformer Model\"\n",
        "        )\n",
        "\n",
        "    def test_scheduler(self):\n",
        "        model = TransformerModel(ModelConfig())\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = CustomScheduler(optimizer, 10_000)\n",
        "        optimizer.step()\n",
        "        scheduler.step(521)\n",
        "        self.check_correctness(\n",
        "            torch.tensor([optimizer.param_groups[0]['lr']]),\n",
        "            self.gt_vars['scheduler_1'],\n",
        "            f\"Scheduler Warmup\"\n",
        "        )\n",
        "        scheduler.step(2503)\n",
        "        self.check_correctness(\n",
        "            torch.tensor([optimizer.param_groups[0]['lr']]),\n",
        "            self.gt_vars['scheduler_2'],\n",
        "            f\"Scheduler Cooldown\"\n",
        "        )\n",
        "\n",
        "    def test_loss(self):\n",
        "        seed_all()\n",
        "        model = TransformerModel(ModelConfig())\n",
        "        trainer = Trainer(model, TrainerConfig(device='cpu'))\n",
        "        loss_result, _ = trainer.compute_batch_loss_acc(\n",
        "            torch.randn((8, 12, 5)),\n",
        "            torch.ones(8).float(),\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            loss_result,\n",
        "            self.gt_vars['loss'],\n",
        "            f\"Batch Loss\"\n",
        "        )\n",
        "\n",
        "    def check_correctness(self, out, gt, title):\n",
        "        try:\n",
        "            diff = (out - gt).norm()\n",
        "        except:\n",
        "            diff = float('inf')\n",
        "        if diff < 1e-4:\n",
        "            print(f\"[Correct] {title}\")\n",
        "        else:\n",
        "            print(f\"[Wrong] {title}\")\n",
        "            if self.verbose:\n",
        "                print(\"-----\")\n",
        "                print(\"Expected: \")\n",
        "                print(gt)\n",
        "                print(\"Received: \")\n",
        "                print(out)\n",
        "                print(\"-----\")\n"
      ],
      "metadata": {
        "id": "UjRY9u_UsFNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1-2-__6AALEfqhfew3sJ2QiCE1-rrFMnQ -q -O unit_tests.pkl\n",
        "import pickle\n",
        "with open('unit_tests.pkl', 'rb') as f:\n",
        "    gt_vars = pickle.load(f)"
      ],
      "metadata": {
        "id": "u2DlMVJ4wMrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TransformerUnitTest(gt_vars, verbose=False).test_all()"
      ],
      "metadata": {
        "id": "Enp2ArbjOHEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7xZShkKqex04"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}